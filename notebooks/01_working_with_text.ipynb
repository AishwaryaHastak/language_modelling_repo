{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Working with Text**\n",
    "\n",
    "Getting text ready to be processed by Languange Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Tokenize the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use A Tale of Two Cities by Charles Dickens from the Gutenberg Project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"http://www.gutenberg.org/files/98/98-0.txt\"\n",
    "\n",
    "response = requests.get(url)\n",
    "text = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of A Tale of Two Cities, by Charles Dickens\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with alm\n"
     ]
    }
   ],
   "source": [
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "793331\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split text on puctuations and white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "words = re.split('\\s', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'A', 'Tale', 'of', 'Two', 'Cities,', 'by', 'Charles', 'Dickens', '', '', '', 'This', 'eBook', 'is', 'for']\n"
     ]
    }
   ],
   "source": [
    "print(words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The BOM (\\ufeff), a Unicode character (the Byte Order Mark, or BOM) at the beginning, is often used to indicate the encoding of a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the BOM using regular expression\n",
    "text = re.sub(r'^\\ufeff', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split on white space and punctations like commas, periods, apostrophes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '\"', 'this', '\"', 'is', '-', '-', 'a', 'sample', 'text', '.', 'There', \"'\", 's', 'a', 'second', 'line', '!']\n"
     ]
    }
   ],
   "source": [
    "sample_text = r\"\"\"Hello, \"this\" is-- a sample text. There's a second line!\"\"\"\n",
    "sample_words = re.split(r'([,.:;?_!\"()\\'-])|\\s', sample_text)\n",
    "# Filter out empty strings\n",
    "sample_words = [word for word in sample_words if word]\n",
    "print(sample_words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Project', 'Gutenberg', 'eBook', 'of', 'A', 'Tale', 'of', 'Two', 'Cities', ',', 'by', 'Charles', 'Dickens', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other']\n"
     ]
    }
   ],
   "source": [
    "words = re.split(r'([,.:;?_!\"()\\'-])|\\s', text)\n",
    "\n",
    "# Filter out empty strings\n",
    "words = [word for word in words if word]\n",
    "\n",
    "print(words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169846\n"
     ]
    }
   ],
   "source": [
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Convert tokens to token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a vocabulary using the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['accordance', 'industrious', 'engaged', 'â€œhere', 'brigand']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = list(set(words))\n",
    "print(len(unique_words))\n",
    "unique_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {word: index for index, word in enumerate(unique_words)}\n",
    "print(len(vocab))\n",
    "vocab['engaged']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a **Simple Tokenizer** class that encodes and decodes the input string based on our vocabulary\n",
    "- Vocab has a mapping from word -> index.\n",
    "- We need to create a reverse_lookup mapping needed for decoding the tokens that maps index -> words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.reverse_lookup = {i: w for w, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = re.split(r'([,.:;?_!\"()\\'-])|\\s', text)\n",
    "        # Filter out empty strings\n",
    "        tokens = [token for token in tokens if token]\n",
    "        return [self.vocab[token] for token in tokens]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return ' '.join([self.reverse_lookup[token] for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = r'A Tale of Cities - by Charles Dickens'\n",
    "tokenizer = SimpleTokenizer(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [3154, 1347, 8772, 11105, 5182, 6340, 9808, 1877]\n",
      "Decoded: A Tale of Cities - by Charles Dickens\n"
     ]
    }
   ],
   "source": [
    "print('Encoded:', tokenizer.encode(sample_text))\n",
    "print('Decoded:', tokenizer.decode(tokenizer.encode(sample_text)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
