{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Working with Text**\n",
    "\n",
    "Getting text ready to be processed by Languange Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Tokenize the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use A Tale of Two Cities by Charles Dickens from the Gutenberg Project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"http://www.gutenberg.org/files/98/98-0.txt\"\n",
    "\n",
    "response = requests.get(url)\n",
    "text = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of A Tale of Two Cities, by Charles Dickens\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with alm\n"
     ]
    }
   ],
   "source": [
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "793331\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split text on puctuations and white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "words = re.split('\\s', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'A', 'Tale', 'of', 'Two', 'Cities,', 'by', 'Charles', 'Dickens', '', '', '', 'This', 'eBook', 'is', 'for']\n"
     ]
    }
   ],
   "source": [
    "print(words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The BOM (\\ufeff), a Unicode character (the Byte Order Mark, or BOM) at the beginning, is often used to indicate the encoding of a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the BOM using regular expression\n",
    "text = re.sub(r'^\\ufeff', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split on white space and punctations like commas, periods, apostrophes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '\"', 'this', '\"', 'is', '-', '-', 'a', 'sample', 'text', '.', 'There', \"'\", 's', 'a', 'second', 'line', '!']\n"
     ]
    }
   ],
   "source": [
    "sample_text = r\"\"\"Hello, \"this\" is-- a sample text. There's a second line!\"\"\"\n",
    "sample_words = re.split(r'([,.:;?_!\"()\\'-])|\\s', sample_text)\n",
    "# Filter out empty strings\n",
    "sample_words = [word for word in sample_words if word]\n",
    "print(sample_words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Project', 'Gutenberg', 'eBook', 'of', 'A', 'Tale', 'of', 'Two', 'Cities', ',', 'by', 'Charles', 'Dickens', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other']\n"
     ]
    }
   ],
   "source": [
    "words = re.split(r'([,.:;?_!\"()\\'-])|\\s', text)\n",
    "\n",
    "# Filter out empty strings\n",
    "words = [word for word in words if word]\n",
    "\n",
    "print(words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169846\n"
     ]
    }
   ],
   "source": [
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Convert tokens to token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of unique tokens that an NLP system operates on is called a vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a vocabulary using the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['An', 'And', 'Angel', 'Angels', 'Angel’s']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = sorted(list(set(words)))\n",
    "print(len(unique_words))\n",
    "unique_words[100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {word: index for index, word in enumerate(unique_words)}\n",
    "print(len(vocab))\n",
    "vocab['Angel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Tokenizer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a **Simple Tokenizer** class that encodes and decodes the input string based on our vocabulary\n",
    "- Vocab has a mapping from word -> index.\n",
    "- We need to create a reverse_lookup mapping needed for decoding the tokens that maps index -> words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.reverse_lookup = {i: w for w, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = re.split(r'([,.:;?_!\"()\\'-])|\\s', text)\n",
    "        # Filter out empty strings\n",
    "        tokens = [token for token in tokens if token]\n",
    "        return [self.vocab[token] for token in tokens]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return ' '.join([self.reverse_lookup[token] for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = r'A Tale of Cities - by Charles Dickens'\n",
    "tokenizer = SimpleTokenizer(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [52, 1221, 7122, 254, 12, 2512, 240, 360]\n",
      "Decoded: A Tale of Cities - by Charles Dickens\n"
     ]
    }
   ],
   "source": [
    "print('Encoded:', tokenizer.encode(sample_text))\n",
    "print('Decoded:', tokenizer.decode(tokenizer.encode(sample_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Adding Special tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding some special tokens in the vocabulary gives the LLM extra context. Such as:\n",
    "- [SOS] Start of sequence, denotes the beginning of a sequence\n",
    "- [EOS] End of sequence, denote the end of a sequence or between concatenated texts like between multiple articles, books,etc.\n",
    "- [PAD] Padding token, is used to match the expected input length to the LLM when the actual input length is smaller than the expected sequence length\n",
    "- [UNK] Unknown token, denote out-of-vocabulary tokens that did not occur in the vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Maybe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[159], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m sample_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaybe these words do not exist in the vocab\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m SimpleTokenizer(vocab)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[155], line 10\u001b[0m, in \u001b[0;36mSimpleTokenizer.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Filter out empty strings\u001b[39;00m\n\u001b[0;32m      9\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token]\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab[token] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "Cell \u001b[1;32mIn[155], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Filter out empty strings\u001b[39;00m\n\u001b[0;32m      9\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token]\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Maybe'"
     ]
    }
   ],
   "source": [
    "sample_text = r'Maybe these words do not exist in the vocab'\n",
    "tokenizer = SimpleTokenizer(vocab)\n",
    "tokenizer.encode(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The cell above returns an error since the word 'Maybe' is not present in our vocabulary. This can be handled by adding a [UNK] token for any out-of-vocab token\n",
    "- We can also add the [EOS] token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"<|EOS|>\", \"<|UNK|>\"]\n",
    "unique_words.extend(special_tokens)\n",
    "vocab = {word: index for index, word in enumerate(unique_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“‘You', '“‘the', '”', '<|EOS|>', '<|UNK|>']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“‘You 11655\n",
      "“‘the 11656\n",
      "” 11657\n",
      "<|EOS|> 11658\n",
      "<|UNK|> 11659\n"
     ]
    }
   ],
   "source": [
    "for token,id in list(vocab.items())[-5:]:\n",
    "    print(token, id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Modify the Tokenizer class to include special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.reverse_lookup = {i: w for w, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = re.split(r'([,.:;?_!\"()\\'-])|\\s', text)\n",
    "        # Filter out empty strings\n",
    "        tokens = [token for token in tokens if token]\n",
    "        return [self.vocab[token] if token in self.vocab else self.vocab['<|UNK|>'] for token in tokens]\n",
    "\n",
    "    def decode(self, tokens): \n",
    "        return ' '.join([self.reverse_lookup[token] for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enocded: [11659, 9960, 10962, 3925, 7026, 4438, 5770, 9938, 11659, 13, 11658, 52, 1221, 7122, 254, 12, 2512, 240, 360, 13]\n",
      "Decoded: <|UNK|> these words do not exist in the <|UNK|> . <|EOS|> A Tale of Cities - by Charles Dickens .\n"
     ]
    }
   ],
   "source": [
    "sample_text1 = r'Maybe these words do not exist in the vocabulary.'\n",
    "sample_text2 = r'A Tale of Cities - by Charles Dickens.'\n",
    "sample_text = sample_text1 + '<|EOS|> ' + sample_text2\n",
    "\n",
    "tokenizer = SimpleTokenizer(vocab)\n",
    "print('Enocded:', tokenizer.encode(sample_text)) \n",
    "print('Decoded:', tokenizer.decode(tokenizer.encode(sample_text)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
